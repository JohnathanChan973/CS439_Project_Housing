{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.exc import GeocoderTimedOut, GeocoderUnavailable\n",
    "from geopy.distance import geodesic\n",
    "import time\n",
    "import sqlite3\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.cm as cm\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_DIR = Path(\"data\")\n",
    "PATH_NEIGHBORHOOD = DATA_DIR / \"Neighborhood_zhvi_uc_sfrcondo_tier_0.33_0.67_sm_sa_month.csv\"\n",
    "PATH_METRO_ZHVI = DATA_DIR / \"Metro_zhvi_uc_sfrcondo_tier_0.33_0.67_sm_sa_month.csv\"\n",
    "PATH_METRO_SALES = DATA_DIR / \"Metro_median_sale_price_uc_sfrcondo_sm_week.csv\"\n",
    "PATH_METRO_SALES_RAW = DATA_DIR / \"Metro_median_sale_price_uc_sfr_month.csv\"\n",
    "PATH_LOC = DATA_DIR / \"Location_Data.json\"\n",
    "PATH_METRO_LOC = DATA_DIR / \"Metro_Location_Data.json\"\n",
    "SQLITE_DB_URL = Path(\"zillow_stats.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def melt_dataframes(df : pd.DataFrame, static_data : list, primary_key : str, value_name : str, var_name : str = \"Date\"):\n",
    "    \"\"\"\n",
    "    Separates time series data from the data that does not change over time and melts the time series data\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The df containing the csv data\n",
    "        static_data (list): A list of strings corresponding to the df column names that will not change\n",
    "        primary_key (str): The column name of the primary key, which will be included in both returned DataFrames\n",
    "        value_name (str): The name of the new column of melted data\n",
    "        var_name (str): The column name on which the df will be melted on\n",
    "\n",
    "    Returns:\n",
    "        tuple:\n",
    "            df_data (pd.DataFrame): A df containing all the data that will not change\n",
    "            df_values (pd.DataFrame): A df containing all the melted data\n",
    "    \"\"\"\n",
    "    static_copy = static_data.copy() # Needs to create a copy because Python, otherwise it changes the passed in version\n",
    "    df_data = df[static_copy].drop_duplicates().reset_index(drop=True) # The drop_duplicates is not actually necessary, but good just in case\n",
    "    if primary_key in static_copy:\n",
    "        static_copy.remove(primary_key) # Used for making the other table later\n",
    "    df_data[static_copy]=df_data[static_copy].astype('string') # Makes the data types of all columns except primary key strings instead of objects\n",
    "    df_values = df.drop(columns=static_copy) # Removes the static data from the df, so just the cols we want melted down\n",
    "    df_values = pd.melt(df_values, primary_key, var_name=var_name, value_name=value_name)\n",
    "    if var_name == \"Date\":\n",
    "        df_values[var_name] = pd.to_datetime(df_values[var_name]).dt.strftime(\"%Y-%m-%d\") # Turns Date col into string rather than object to work better with SQLite\n",
    "    df_values = df_values.dropna(subset=value_name) # Remove rows with empty data in the value col\n",
    "    return df_data, df_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(PATH_NEIGHBORHOOD)\n",
    "\n",
    "# These will be the columns of one of the tables\n",
    "static_data = [\"RegionID\", \"RegionName\", \"RegionType\", \"StateName\", \"City\", \"Metro\", \"CountyName\"]\n",
    "\n",
    "df.drop(columns=['State', 'SizeRank'], inplace=True) # Removes State and keeps StateName since they are completely identical\n",
    "# SizeRank is 0 indexed, has duplicate values, and has a max value of 28019, which doesn't make sense with only 21636 rows.\n",
    "# I don't understand its value and didn't plan on using it anways, so may as well get rid of it\n",
    "\n",
    "df_data, df_values = melt_dataframes(df, static_data, \"RegionID\", \"ZHVI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coord_df = pd.read_json(PATH_LOC)\n",
    "df_data = df_data.set_index(\"RegionID\").join(coord_df.T).reset_index() # Adds the Latitude and Longitude from the json to the df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_values.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metro_zhvi_df = pd.read_csv(PATH_METRO_ZHVI)\n",
    "metro_sales_df = pd.read_csv(PATH_METRO_SALES)\n",
    "metro_sales_raw_df = pd.read_csv(PATH_METRO_SALES_RAW)\n",
    "metro_sales_raw_df.drop(columns=['SizeRank'], inplace=True)\n",
    "metro_zhvi_df.drop(columns=['SizeRank'], inplace=True)\n",
    "metro_sales_df.drop(columns=['SizeRank'], inplace=True)\n",
    "\n",
    "static_loc_data = [\"RegionID\", \"RegionName\", \"RegionType\", \"StateName\"]\n",
    "primary_key = \"RegionID\"\n",
    "metro_loc_sales, metro_time_sales = melt_dataframes(metro_sales_df, static_loc_data, primary_key, \"Median_Sales\")\n",
    "metro_loc_zhvi, metro_time_zhvi = melt_dataframes(metro_zhvi_df, static_loc_data, primary_key, \"ZHVI\")\n",
    "metro_loc_sales_raw, metro_time_sales_raw = melt_dataframes(metro_sales_raw_df, static_loc_data, primary_key, \"Median_Sales_Raw\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_metro_loc = pd.concat([metro_loc_sales, metro_loc_zhvi, metro_loc_sales_raw], ignore_index=True).drop_duplicates() # Have to combine since all in one not necessarily in other\n",
    "full_metro_loc.loc[full_metro_loc[\"RegionID\"] == 102001, \"StateName\"] = \"USA\" # StateName is initally NaN for the United States\n",
    "metro_coord_df = pd.read_json(PATH_METRO_LOC)\n",
    "full_metro_loc = full_metro_loc.set_index(\"RegionID\").join(metro_coord_df.T).reset_index() # Adds the Latitude and Longitude from the json to the df\n",
    "full_metro_loc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metro_time_sales.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metro_time_zhvi.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metro_time_sales_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This class was used for preprocessing to get all the coords, taking >10000 minutes (even longer with the failed attempts). Do not run again if not needed \n",
    "class ZillowGeocoder:\n",
    "    def __init__(self, user_agent : str=\"zillow_housing_plotting_project\", timeout : int=10, rate_limit_delay : int=1):\n",
    "        \"\"\"\n",
    "        Initializes the geocoder\n",
    "        \n",
    "        Parameters:\n",
    "            user_agent (str): User agent for Nominatim\n",
    "            timeout (int): Timeout in seconds for geocoding requests\n",
    "            rate_limit_delay (int): Delay between requests in seconds\n",
    "        \"\"\"\n",
    "        self.geolocator = Nominatim(user_agent=user_agent, timeout=timeout)\n",
    "        self.rate_limit_delay = rate_limit_delay\n",
    "        self.city_coords_cache = {}  # Cache for city coordinates\n",
    "    \n",
    "    def geocode(self, address : str, retry_count : int=0, max_retries : int=1):\n",
    "        \"\"\"\n",
    "        Geocodes a given address\n",
    "        \n",
    "        Parameters:\n",
    "            address (str): Address to geocode\n",
    "            retry_count (int): Current retry attempt\n",
    "            max_retries (int): Maximum number of retries\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (latitude, longitude) or (None, None) if geocoding fails\n",
    "        \"\"\"\n",
    "        try:\n",
    "            loc = self.geolocator.geocode(address)\n",
    "            if loc:\n",
    "                return loc.latitude, loc.longitude\n",
    "            else:\n",
    "                print(f\"No results found for {address}\")\n",
    "                return None, None\n",
    "                \n",
    "        except GeocoderTimedOut:\n",
    "            if retry_count < max_retries:\n",
    "                print(f\"Geocoding timed out for {address}, retrying... ({retry_count+1}/{max_retries})\")\n",
    "                time.sleep(2)\n",
    "                return self.geocode(address, retry_count + 1, max_retries) # Calls itself recursively\n",
    "            else:\n",
    "                print(f\"Max retries reached for {address}\")\n",
    "                return None, None\n",
    "        except GeocoderUnavailable:\n",
    "            print(f\"Geocoding service unavailable for {address}\")\n",
    "            return None, None\n",
    "        except Exception as e:\n",
    "            print(f\"Error geocoding {address}: {e}\")\n",
    "            return None, None\n",
    "    \n",
    "    def geocode_metro(self, region_name : str):\n",
    "        \"\"\"\n",
    "        Geocode a metro region name\n",
    "        \n",
    "        Parameters:\n",
    "            region_name (str): Name of the metro region\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (latitude, longitude) or (None, None) if geocoding fails\n",
    "        \"\"\"\n",
    "        return self.geocode(region_name)\n",
    "    \n",
    "    def geocode_region(self, location : str, state : str, city : str):\n",
    "        \"\"\"\n",
    "        Geocode a region and uses the city level as a fallback if the region geocoding fails\n",
    "        \n",
    "        Parameters:\n",
    "            location (str): Region/neighborhood name\n",
    "            state (str): State name\n",
    "            city (str): City name for fallback\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (latitude, longitude) or (None, None) if geocoding fails\n",
    "        \"\"\"\n",
    "        # Try specific location first\n",
    "        full_address = f\"{location}, {state}\"\n",
    "        lat, lon = self.geocode(full_address)\n",
    "        \n",
    "        # Fallback to city level if needed\n",
    "        if lat is None and location != city: # No need to check lon since if lat is None, both are None\n",
    "            print(f\"Falling back to city level for {full_address}\")\n",
    "            broad_address = f\"{city}, {state}\"\n",
    "            lat, lon = self.geocode(broad_address)\n",
    "            \n",
    "        return lat, lon\n",
    "    \n",
    "    def add_coordinates_to_df(self, df : pd.DataFrame, region_col : str=\"RegionName\", state_col : str=\"StateName\", city_col : str=None, method : str=\"region\"):\n",
    "        \"\"\"\n",
    "        Add latitude and longitude coordinates to the given df\n",
    "        \n",
    "        Parameters:\n",
    "            df (pd.DataFrame): DataFrame containing location data\n",
    "            region_col (str): Name of column in df containing region names\n",
    "            state_col (str): Name of column in df containing state names\n",
    "            city_col (str, optional): Name of column in df containing city names \n",
    "            method (str): \"metro\", \"region\", or \"city\" - determines geocoding approach\n",
    "            \n",
    "        Returns:\n",
    "            pd.DataFrame: DataFrame with added Latitude and Longitude columns\n",
    "        \"\"\"\n",
    "        result_df = df.copy() # Creates a copy to avoid the df passed in\n",
    "        \n",
    "        # Lists to hold geocoded results\n",
    "        latitudes = []\n",
    "        longitudes = []\n",
    "        \n",
    "        print(f'Starting geocoding for {len(df)} locations using {method} method')\n",
    "        \n",
    "        # Process each row in the DataFrame\n",
    "        for idx, row in df.iterrows():\n",
    "            if idx % 50 == 0 and idx > 0:\n",
    "                print(f\"Processed {idx}/{len(df)} locations\") # Just to keep track of how many are done\n",
    "                \n",
    "            if method == \"metro\":\n",
    "                lat, lon = self.geocode_metro(row[region_col]) # Metro-level geocoding\n",
    "            elif method == \"region\" and city_col:\n",
    "                lat, lon = self.geocode_region(row[region_col], row[state_col], row[city_col]) # Region-level geocoding with city fallback\n",
    "            elif method == \"city\":\n",
    "                address = f\"{row[city_col]}, {row[state_col]}\"\n",
    "                lat, lon = self.geocode(address) # City-level geocoding\n",
    "            else:\n",
    "                address = f\"{row[region_col]}, {row[state_col]}\"\n",
    "                lat, lon = self.geocode(address) # Default geocoding approach\n",
    "            \n",
    "            latitudes.append(lat)\n",
    "            longitudes.append(lon)\n",
    "            \n",
    "            time.sleep(self.rate_limit_delay) # Sleeps to avoid rate limits\n",
    "        \n",
    "        # Add coordinates to DataFrame\n",
    "        result_df['Latitude'] = latitudes\n",
    "        result_df['Longitude'] = longitudes\n",
    "        \n",
    "        print(f\"Geocoding complete. Successfully geocoded {sum(lat is not None for lat in latitudes)} out of {len(latitudes)} locations\")\n",
    "        \n",
    "        return result_df\n",
    "    \n",
    "    def build_city_coordinates_cache(self, df : pd.DataFrame, city_col : str=\"City\", state_col : str=\"StateName\"):\n",
    "        \"\"\"\n",
    "        Build a cache of city center coordinates to minimize API calls\n",
    "        \n",
    "        Parameters:\n",
    "            df (pd.DataFrame): DataFrame containing city and state data\n",
    "            city_col (str): Name of column in df containing city names\n",
    "            state_col (str): Name of column in df containing state names\n",
    "            \n",
    "        Returns:\n",
    "            dict: Dictionary mapping \"City, State\" to (latitude, longitude)\n",
    "        \"\"\"\n",
    "        # Create unique city-state combinations\n",
    "        unique_addresses = df[[city_col, state_col]].drop_duplicates().dropna()\n",
    "        print(f\"Need to geocode {len(unique_addresses)} unique city-state combinations\")\n",
    "        \n",
    "        # Geocode each unique city-state combination\n",
    "        for _, row in unique_addresses.iterrows():\n",
    "            address = f\"{row[city_col]}, {row[state_col]}\"\n",
    "            if address not in self.city_coords_cache:  # Check if already in cache\n",
    "                coords = self.geocode(address)\n",
    "                if coords[0] is not None:\n",
    "                    self.city_coords_cache[address] = coords\n",
    "                time.sleep(self.rate_limit_delay)\n",
    "        \n",
    "        print(f\"Successfully geocoded {len(self.city_coords_cache)} cities\")\n",
    "        return self.city_coords_cache\n",
    "    \n",
    "    def find_misplaced_regions(self, df : pd.DataFrame, distance_threshold : int=250, \n",
    "                              city_col : str=\"City\", state_col : str=\"StateName\", \n",
    "                              lat_col : str=\"Latitude\", lon_col : str=\"Longitude\", \n",
    "                              id_col : str=\"RegionID\"):\n",
    "        \"\"\"\n",
    "        Find regions that are too far from their city centers\n",
    "        \n",
    "        Parameters:\n",
    "            df (pd.DataFrame): DataFrame with coordinates and city information\n",
    "            distance_threshold (int): Maximum allowed distance in miles from city center\n",
    "            city_col (str): Name of column in df containing city names\n",
    "            state_col (str): Name of column in df containing state names\n",
    "            lat_col (str): Name of column in df containing latitudes\n",
    "            lon_col (str): Name of column in df containing longitudes\n",
    "            id_col (str): Name of column in df containing region IDs\n",
    "            \n",
    "        Returns:\n",
    "            dict: Dictionary mapping region IDs to corrected city center coordinates\n",
    "        \"\"\"\n",
    "        # Ensure we have city coordinates\n",
    "        if not self.city_coords_cache:\n",
    "            self.build_city_coordinates_cache(df, city_col, state_col)\n",
    "        \n",
    "        # Find misplaced regions\n",
    "        region_id_coords = {}\n",
    "        count = 0\n",
    "        \n",
    "        for _, row in df.iterrows():\n",
    "            # Skip rows with missing data\n",
    "            if (pd.isna(row[city_col]) or pd.isna(row[state_col]) or \n",
    "                pd.isna(row[lat_col]) or pd.isna(row[lon_col])):\n",
    "                continue\n",
    "                \n",
    "            address = f\"{row[city_col]}, {row[state_col]}\"\n",
    "            if address not in self.city_coords_cache:\n",
    "                continue  # Skip if city not in cache\n",
    "                \n",
    "            # Calculate distance from city center\n",
    "            try:\n",
    "                current_coords = (row[lat_col], row[lon_col])\n",
    "                city_center = self.city_coords_cache[address]\n",
    "                distance = geodesic(current_coords, city_center).miles\n",
    "                \n",
    "                if distance > distance_threshold:\n",
    "                    region_id_coords[row[id_col]] = city_center\n",
    "                    count += 1\n",
    "            except ValueError:\n",
    "                # Handle invalid coordinate values\n",
    "                continue\n",
    "        \n",
    "        print(f\"Found {count} regions more than {distance_threshold} miles from their city centers\")\n",
    "        return region_id_coords\n",
    "    \n",
    "    def correct_misplaced_regions(self, df : pd.DataFrame, misplaced_regions : dict, \n",
    "                                 id_col : str=\"RegionID\", lat_col : str=\"Latitude\", lon_col : str=\"Longitude\"):\n",
    "        \"\"\"\n",
    "        Correct coordinates for misplaced regions\n",
    "        \n",
    "        Parameters:\n",
    "            df (pd.DataFrame): DataFrame with region coordinates\n",
    "            misplaced_regions (dict): Dictionary mapping region IDs to corrected coordinates\n",
    "            id_col (str): Name of columns in df containing region IDs\n",
    "            lat_col (str): Name of columns in df containing latitudes\n",
    "            lon_col (str): Name of columns in df containing longitudes\n",
    "            \n",
    "        Returns:\n",
    "            pd.DataFrame: DataFrame with corrected coordinates\n",
    "        \"\"\"\n",
    "        result_df = df.copy()\n",
    "        \n",
    "        for region_id, coords in misplaced_regions.items():\n",
    "            mask = result_df[id_col] == region_id\n",
    "            result_df.loc[mask, lat_col] = coords[0]\n",
    "            result_df.loc[mask, lon_col] = coords[1]\n",
    "        \n",
    "        print(f\"Corrected coordinates for {len(misplaced_regions)} regions\")\n",
    "        return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DBManager: # All these functions access the db using the path, so may as well make a class\n",
    "    def __init__(self, db_path=SQLITE_DB_URL):\n",
    "        \"\"\"\n",
    "        Initializes using the given DB URL\n",
    "\n",
    "        Parameters:\n",
    "            db_path (str, default=SQLITE_DB_URL): Path to the SQLite database\n",
    "        \"\"\"\n",
    "        self.db_path = db_path\n",
    "        \n",
    "    def table_to_df(self, table_name : str, cols : list=None):\n",
    "        \"\"\"\n",
    "        Load data from a SQLite table into a pandas DataFrame\n",
    "        \n",
    "        Parameters:\n",
    "            table_name (str): Name of the table to query\n",
    "            cols (list, optional): Specific columns to select (defaults to all columns)\n",
    "            \n",
    "        Returns:\n",
    "            pd.DataFrame: A DataFrame containing the queried data\n",
    "        \"\"\"\n",
    "        with sqlite3.connect(self.db_path) as conn:\n",
    "            # Check that the table exists\n",
    "            cursor = conn.cursor()\n",
    "            cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "            valid_tables = {row[0] for row in cursor.fetchall()}\n",
    "            \n",
    "            if table_name not in valid_tables:\n",
    "                raise ValueError(f\"Table '{table_name}' does not exist in the database\")\n",
    "            \n",
    "            # If cols is specified, check that all of them exist in the table\n",
    "            if cols:\n",
    "                cursor.execute(f\"PRAGMA table_info({table_name});\")\n",
    "                table_columns = {row[1] for row in cursor.fetchall()}\n",
    "                \n",
    "                invalid_cols = [col for col in cols if col not in table_columns]\n",
    "                if invalid_cols:\n",
    "                    raise ValueError(f\"Invalid column(s) for table '{table_name}': {', '.join(invalid_cols)}\")\n",
    "                \n",
    "                columns_str = \", \".join(cols)\n",
    "                query = f\"SELECT {columns_str} FROM {table_name}\" # Creates the query with the specified columns\n",
    "            else:\n",
    "                query = f\"SELECT * FROM {table_name}\" # No cols specified, so selects them all\n",
    "            \n",
    "            return pd.read_sql_query(query, conn)\n",
    "\n",
    "    def select_date_table(self, table_name : str, cols : list=None, date : str=\"newest\"):\n",
    "        \"\"\"\n",
    "        Load data from a SQLite table with dates into a pandas DataFrame\n",
    "        \n",
    "        Parameters:\n",
    "            table_name (str): Name of the table to query\n",
    "            cols (list, optional): Specific columns to select (defaults to all columns)\n",
    "            date (str, optional, default=\"newest\"): \"newest\", \"oldest\", or a specific date string to filter by\n",
    "            \n",
    "        Returns:\n",
    "            pd.DataFrame: A DataFrame containing the queried data\n",
    "        \"\"\"\n",
    "        with sqlite3.connect(self.db_path) as conn:\n",
    "            cursor = conn.cursor()\n",
    "            \n",
    "            # Checks that the table exists\n",
    "            cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "            valid_tables = {row[0] for row in cursor.fetchall()}\n",
    "            \n",
    "            if table_name not in valid_tables:\n",
    "                raise ValueError(f\"Table '{table_name}' does not exist in the database\")\n",
    "            \n",
    "            # Gets all columns from the table\n",
    "            cursor.execute(f\"PRAGMA table_info({table_name});\")\n",
    "            table_columns = {row[1] for row in cursor.fetchall()}\n",
    "            \n",
    "            # Checks that the Date column exists in the table\n",
    "            if \"Date\" not in table_columns:\n",
    "                raise ValueError(f\"Invalid table. There is no 'Date' column in '{table_name}'\")\n",
    "            \n",
    "            # Handles date filtering\n",
    "            date_condition = \"\"\n",
    "            if date == \"newest\":\n",
    "                date_query = f\"SELECT MAX(Date) FROM {table_name}\"\n",
    "                cursor.execute(date_query)\n",
    "                max_date = cursor.fetchone()[0]\n",
    "                date_condition = f\"WHERE Date = '{max_date}'\"\n",
    "            elif date == \"oldest\":\n",
    "                date_query = f\"SELECT MIN(Date) FROM {table_name}\"\n",
    "                cursor.execute(date_query)\n",
    "                min_date = cursor.fetchone()[0]\n",
    "                date_condition = f\"WHERE Date = '{min_date}'\"\n",
    "            elif date:\n",
    "                date_condition = f\"WHERE Date = '{date}'\"\n",
    "            \n",
    "            # Handles column selection\n",
    "            if cols:\n",
    "                invalid_cols = [col for col in cols if col not in table_columns]\n",
    "                if invalid_cols:\n",
    "                    raise ValueError(f\"Invalid column(s) for table '{table_name}': {', '.join(invalid_cols)}\")\n",
    "                \n",
    "                columns_str = \", \".join(cols)\n",
    "            else:\n",
    "                columns_str = \"*\"\n",
    "            \n",
    "            # Creates the query\n",
    "            query = f\"\"\"\n",
    "                SELECT {columns_str}\n",
    "                FROM {table_name}\n",
    "                {date_condition}\n",
    "            \"\"\"\n",
    "            \n",
    "            return pd.read_sql_query(query, conn)\n",
    "        \n",
    "    def select_merged_table(self, table_names : str, key : str=\"RegionID\", key_value : str=None, cols : list=None, date : str=None):\n",
    "        \"\"\"\n",
    "        Load data from multiple SQLite tables merged on a common key with optional date filtering\n",
    "        \n",
    "        Parameters:\n",
    "            table_names (set or list): Names of the tables to be merged and queried\n",
    "            key (str, default=\"RegionID\"): The column name to join tables on\n",
    "            cols (list or tuple, optional): Specific columns to select (defaults to all columns)\n",
    "            date (str, optional): \"newest\", \"oldest\", or a specific date string to filter by\n",
    "            \n",
    "        Returns:\n",
    "            pd.DataFrame: A DataFrame containing the merged data\n",
    "        \"\"\"\n",
    "        table_list = list(table_names) # Converts to list for easier indexing\n",
    "        \n",
    "        if len(table_list) != 2: # Merging more tables is tough and shouldn't be necessary for the project\n",
    "            raise ValueError(\"Can only merge exactly two tables.\")\n",
    "        \n",
    "        with sqlite3.connect(self.db_path) as conn:\n",
    "            cursor = conn.cursor()\n",
    "            \n",
    "            # Checks that all tables exist\n",
    "            cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "            valid_tables = {row[0] for row in cursor.fetchall()}\n",
    "            \n",
    "            invalid_tables = [table for table in table_list if table not in valid_tables]\n",
    "            if invalid_tables:\n",
    "                raise ValueError(f\"Table(s) '{', '.join(invalid_tables)}' do not exist in the database\")\n",
    "            \n",
    "            # Finds the table with the Date column so it can be filtered properly\n",
    "            date_table = None\n",
    "            for table in table_list:\n",
    "                cursor.execute(f\"PRAGMA table_info({table});\")\n",
    "                columns = {row[1] for row in cursor.fetchall()}\n",
    "                if \"Date\" in columns:\n",
    "                    date_table = table\n",
    "                    break\n",
    "            \n",
    "            if date_table is None:\n",
    "                raise ValueError(\"None of the specified tables have a 'Date' column\")\n",
    "            \n",
    "            # Get all columns from all tables to check against the requested columns\n",
    "            all_columns = {}\n",
    "            for table in table_list:\n",
    "                cursor.execute(f\"PRAGMA table_info({table});\")\n",
    "                all_columns[table] = {row[1] for row in cursor.fetchall()}\n",
    "            \n",
    "            # Validate the join key exists in both tables\n",
    "            for table in table_list:\n",
    "                if key not in all_columns[table]:\n",
    "                    raise ValueError(f\"Join key '{key}' not found in table '{table}'\")\n",
    "            \n",
    "            # Handle date filtering\n",
    "            date_condition = \"\"\n",
    "            if date == \"newest\":\n",
    "                date_query = f\"SELECT MAX(Date) FROM {date_table}\"\n",
    "                cursor.execute(date_query)\n",
    "                max_date = cursor.fetchone()[0]\n",
    "                date_condition = f\"{date_table}.Date = '{max_date}'\"\n",
    "            elif date == \"oldest\":\n",
    "                date_query = f\"SELECT MIN(Date) FROM {date_table}\"\n",
    "                cursor.execute(date_query)\n",
    "                min_date = cursor.fetchone()[0]\n",
    "                date_condition = f\"{date_table}.Date = '{min_date}'\"\n",
    "            elif date:\n",
    "                date_condition = f\"{date_table}.Date = '{date}'\"\n",
    "            \n",
    "            # Handle column selection\n",
    "            if cols:\n",
    "                # Check if specified columns exist in any of the tables\n",
    "                available_cols = set()\n",
    "                for table_cols in all_columns.values():\n",
    "                    available_cols.update(table_cols)\n",
    "                \n",
    "                invalid_cols = [col for col in cols if col not in available_cols]\n",
    "                if invalid_cols:\n",
    "                    raise ValueError(f\"Invalid column(s): {', '.join(invalid_cols)}\")\n",
    "                \n",
    "                # Build column selection with table prefixes to avoid ambiguity\n",
    "                select_cols = []\n",
    "                for col in cols:\n",
    "                    # Find which table(s) have this column\n",
    "                    tables_with_col = [t for t in table_list if col in all_columns[t]]\n",
    "                    if len(tables_with_col) == 1:\n",
    "                        select_cols.append(f\"{tables_with_col[0]}.{col}\")\n",
    "                    elif col == key:  # Join key appears in both tables, pick one\n",
    "                        select_cols.append(f\"{table_list[0]}.{col}\")\n",
    "                    else:  # Column appears in multiple tables, include both with aliases\n",
    "                        for table in tables_with_col:\n",
    "                            select_cols.append(f\"{table}.{col} AS {table}_{col}\")\n",
    "                \n",
    "                columns_str = \", \".join(select_cols)\n",
    "            else:\n",
    "                # Select all columns\n",
    "                columns_str = \"*\"\n",
    "\n",
    "            # Prepare key filter\n",
    "            where_condition = \"\"\n",
    "            if key_value:\n",
    "                where_condition = f\"{key} = {key_value}\"\n",
    "\n",
    "            # Combine conditions (instead of using multiple WHEREs)\n",
    "            conditions = \" AND \".join(filter(None, [date_condition, where_condition]))\n",
    "            where_clause = f\"WHERE {conditions}\" if conditions else \"\"\n",
    "\n",
    "            # Build the query\n",
    "            query = f\"\"\"\n",
    "                SELECT {columns_str}\n",
    "                FROM {table_list[0]}\n",
    "                JOIN {table_list[1]} USING ({key})\n",
    "                {where_clause}\n",
    "            \"\"\"\n",
    "            \n",
    "            return pd.read_sql_query(query, conn)\n",
    "        \n",
    "    def add_to_table(self, table_name : str, df : pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Add or update records in a specified table using data from a DataFrame.\n",
    "        \n",
    "        Parameters:\n",
    "            table_name (str): Name of the target table\n",
    "            df (pd.DataFrame): DataFrame containing data to be added\n",
    "        \n",
    "        Return:\n",
    "            int: Number of rows added\n",
    "        \"\"\"\n",
    "        with sqlite3.connect(self.db_path) as conn:\n",
    "            cursor = conn.cursor()\n",
    "\n",
    "            # Checks if table exists\n",
    "            cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "            valid_tables = {row[0] for row in cursor.fetchall()}\n",
    "            if table_name not in valid_tables:\n",
    "                raise ValueError(f\"Invalid table name: {table_name}\")\n",
    "            \n",
    "            # Gets column information\n",
    "            cursor.execute(f\"PRAGMA table_info({table_name});\")\n",
    "            table_columns = [row[1] for row in cursor.fetchall()]\n",
    "            \n",
    "            # Finds matching columns between the df and table\n",
    "            df_columns = df.columns\n",
    "            common_columns = [col for col in df_columns if col in table_columns]\n",
    "            \n",
    "            if not common_columns:\n",
    "                raise ValueError(f\"No matching columns found between given df and {table_name}\")\n",
    "            \n",
    "            # Creates placeholders for the SQL query\n",
    "            placeholders = \", \".join([\"?\" for _ in common_columns])\n",
    "            columns_str = \", \".join(common_columns)\n",
    "            \n",
    "            # Creates INSERT statement to handle both new records\n",
    "            insert_sql = f\"\"\"\n",
    "                INSERT INTO {table_name} ({columns_str})\n",
    "                VALUES ({placeholders})\n",
    "            \"\"\"\n",
    "            \n",
    "            # Prepares data for insertion\n",
    "            insert_data = [\n",
    "                tuple(row[col] for col in common_columns)\n",
    "                for _, row in df.iterrows()\n",
    "            ]\n",
    "            \n",
    "            # Execute and commit\n",
    "            cursor.executemany(insert_sql, insert_data)\n",
    "            conn.commit()\n",
    "            \n",
    "            return cursor.rowcount\n",
    "    # Way used:\n",
    "    # add_to_table(\"Metro_Location_Data\", metro_loc_sales_raw)\n",
    "    # continental_sales_raw_df[continental_sales_raw_df[\"RegionID\"] == 753924]\n",
    "    # continental_sales_raw_df.loc[continental_sales_raw_df[\"RegionID\"] == 394437, \"RegionName\"] = \"Canon City, CO\"\n",
    "    # continental_sales_raw_df.loc[continental_sales_raw_df[\"RegionID\"] == 394437, \"Latitude\"] = 38.4422506\n",
    "    # continental_sales_raw_df.loc[continental_sales_raw_df[\"RegionID\"] == 394437, \"Longitude\"] = -105.2348795\n",
    "    # continental_sales_raw_df[continental_sales_raw_df[\"RegionID\"] == 394437]\n",
    "    # add_to_table(\"Metro_Location_Data\", continental_sales_raw_df[continental_sales_raw_df[\"RegionID\"] == 394437])\n",
    "\n",
    "    def safe_to_sqlite(self, df: pd.DataFrame, table_name: str):\n",
    "        \"\"\"\n",
    "        Safely converts DataFrame into a SQLite table\n",
    "            \n",
    "        Parameters:\n",
    "            df (pd.DataFrame): The DataFrame that is being added to the db\n",
    "            table_name (str): The name that the table with have after it is added\n",
    "        \"\"\"\n",
    "        with sqlite3.connect(self.db_path) as conn: # Should create the db if it does not already exist\n",
    "            # Checks if table already exists\n",
    "            cursor = conn.execute(\n",
    "                \"SELECT name FROM sqlite_master WHERE type='table' AND name=?;\", (table_name,)\n",
    "            )\n",
    "            exists = cursor.fetchone() is not None\n",
    "\n",
    "            if not exists:\n",
    "                df.to_sql(table_name, conn, index=False)\n",
    "                print(f\"✅ Created '{table_name}'.\")\n",
    "            else:\n",
    "                print(f\"ℹ️ '{table_name}' already exists. Skipping...\")\n",
    "\n",
    "    def drop_column_sqlite(self, table_name : str, column_to_drop : str):\n",
    "        \"\"\"\n",
    "        Drops a column from a table\n",
    "            \n",
    "        Parameters:\n",
    "            table_name (str): The name of the table whose column is being removed\n",
    "            column_to_drop (str): The name of the column that is being removed\n",
    "        \"\"\"\n",
    "        with sqlite3.connect(self.db_path) as conn:\n",
    "            cursor = conn.cursor()\n",
    "\n",
    "            # Gets columns from the table\n",
    "            cursor.execute(f\"PRAGMA table_info({table_name})\")\n",
    "            columns_info = cursor.fetchall()\n",
    "            all_columns = [col[1] for col in columns_info]\n",
    "\n",
    "            if column_to_drop not in all_columns:\n",
    "                raise ValueError(f\"Column '{column_to_drop}' does not exist in table '{table_name}'.\")\n",
    "\n",
    "            columns_to_keep = [col for col in all_columns if col != column_to_drop]\n",
    "            columns_str = ', '.join(columns_to_keep)\n",
    "\n",
    "            # Gets column definitions (name and type) for new table\n",
    "            new_columns_def = ', '.join(\n",
    "                f\"{col[1]} {col[2]}\" for col in columns_info if col[1] != column_to_drop\n",
    "            )\n",
    "\n",
    "            # Creates new table and puts data from old table in\n",
    "            cursor.executescript(f\"\"\"\n",
    "                CREATE TABLE new_{table_name} ({new_columns_def});\n",
    "\n",
    "                INSERT INTO new_{table_name} ({columns_str})\n",
    "                SELECT {columns_str} FROM {table_name};\n",
    "\n",
    "                DROP TABLE {table_name};\n",
    "\n",
    "                ALTER TABLE new_{table_name} RENAME TO {table_name};\n",
    "            \"\"\")\n",
    "\n",
    "            conn.commit()\n",
    "\n",
    "    def update_datetime_column_to_date(self, table_name : str, datetime_column : str):\n",
    "        \"\"\"\n",
    "        Changes a column in the given table from data in a datetime to a date format\n",
    "\n",
    "        Parameters:\n",
    "            table_name (str): The name of the table which needs its column adjusted\n",
    "            datetime_column (str): The name of the column which needs to be adjusted\n",
    "        \"\"\"\n",
    "        with sqlite3.connect(self.db_path) as conn:\n",
    "            cursor = conn.cursor()\n",
    "\n",
    "            # Updates column with datetime to only store the date (YYYY-MM-DD instead of YYYY-MM-DD hours:minutes:seconds:milliseconds) \n",
    "            update_query = f\"\"\"\n",
    "            UPDATE {table_name}\n",
    "            SET {datetime_column} = DATE({datetime_column});\n",
    "            \"\"\"\n",
    "            cursor.execute(update_query)\n",
    "            conn.commit()\n",
    "            \n",
    "    def rename_column_in_sqlite(self, table_name : str, old_column : str, new_column : str):\n",
    "        \"\"\"\n",
    "        Changes the name of the given column in the given table to a new name\n",
    "\n",
    "        Parameters:\n",
    "            table_name (str): The name of the table which needs its column adjusted\n",
    "            old_column (str): The name of the column which needs to be adjusted\n",
    "            new_column (str): The new name of the column\n",
    "        \"\"\"\n",
    "        with sqlite3.connect(self.db_path) as conn:\n",
    "            # Get the table's schema\n",
    "            cursor = conn.cursor()\n",
    "            cursor.execute(f\"PRAGMA table_info({table_name});\")\n",
    "            columns = cursor.fetchall()\n",
    "            \n",
    "            if any(col[1] == old_column for col in columns): # Checks if column exists\n",
    "                # Creates a new table with the updated column name\n",
    "                columns_definition = ', '.join(\n",
    "                    [f'\"{col[1]}\" {col[2]}' if col[1] != old_column else f'\"{new_column}\" {col[2]}'\n",
    "                    for col in columns]\n",
    "                )\n",
    "                cursor.execute(f\"\"\"\n",
    "                    CREATE TABLE new_{table_name} ({columns_definition});\n",
    "                \"\"\")\n",
    "\n",
    "                # Adds all the data into the new table from the old table\n",
    "                cursor.execute(f\"\"\"\n",
    "                    INSERT INTO new_{table_name} SELECT * FROM {table_name};\n",
    "                \"\"\")\n",
    "\n",
    "                # Drops the old table\n",
    "                cursor.execute(f\"DROP TABLE {table_name};\")\n",
    "\n",
    "                # Renames the new table to the old table's name\n",
    "                cursor.execute(f\"ALTER TABLE new_{table_name} RENAME TO {table_name};\")\n",
    "\n",
    "                print(f\"Column {old_column} renamed to {new_column} successfully.\")\n",
    "            else:\n",
    "                print(f\"Column {old_column} does not exist in the table.\")\n",
    "\n",
    "            # Way used:\n",
    "            # rename_column_in_sqlite('Metro_Sales_Data', 'Median Sales', 'Median_Sales')\n",
    "            # rename_column_in_sqlite('Location_Data', 'State', 'StateName')\n",
    "\n",
    "    def coordinates_to_json(self, table_name : str, output_dir: Path=DATA_DIR):\n",
    "        \"\"\"\n",
    "        Extracts RegionID, Latitude, and Longitude from the given table and writes them to a JSON file\n",
    "\n",
    "        Parameters:\n",
    "            table_name (str): The name of the table to extract coordinate data from\n",
    "            output_dir (str, default=DATA_DIR): Directory where the output JSON file will be saved\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If the table does not exist or required columns are missing\n",
    "        \"\"\"\n",
    "        with sqlite3.connect(self.db_path) as conn:\n",
    "            cursor = conn.cursor()\n",
    "\n",
    "            # Check if table exists\n",
    "            cursor.execute(\"\"\"\n",
    "                SELECT name FROM sqlite_master\n",
    "                WHERE type='table' AND name=?;\n",
    "            \"\"\", (table_name,))\n",
    "            if not cursor.fetchone():\n",
    "                raise ValueError(f\"Table '{table_name}' does not exist in the database.\")\n",
    "\n",
    "            # Check for RegionID, Latitude, and Longitude columns since they will be the key and attributes in the json \n",
    "            cursor.execute(f\"PRAGMA table_info({table_name});\")\n",
    "            columns = {col[1] for col in cursor.fetchall()}\n",
    "            required = {\"RegionID\", \"Latitude\", \"Longitude\"}\n",
    "            if not required.issubset(columns):\n",
    "                raise ValueError(f\"Table '{table_name}' must contain columns: {required}\")\n",
    "\n",
    "            # Fetch data\n",
    "            cursor.execute(f\"\"\"\n",
    "                SELECT RegionID, Latitude, Longitude FROM {table_name}\n",
    "                WHERE Latitude IS NOT NULL AND Longitude IS NOT NULL;\n",
    "            \"\"\")\n",
    "            rows = cursor.fetchall()\n",
    "\n",
    "            coord_dict = {\n",
    "                str(region_id): {\"Latitude\": lat, \"Longitude\": lon}\n",
    "                for region_id, lat, lon in rows\n",
    "            }\n",
    "\n",
    "            # Write to file\n",
    "            output_dir = Path(output_dir) # Casts to a Path object because Python isn't strongly typed\n",
    "            output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            output_path = output_dir / f\"{table_name}.json\"\n",
    "\n",
    "            with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(coord_dict, f, indent=2)\n",
    "\n",
    "            print(f\"Coordinate data written to '{output_path}'\")\n",
    "\n",
    "    def update_table_coords(self, full_update_df : pd.DataFrame, table_name : str):\n",
    "        \"\"\"\n",
    "        Updates a row's Latitude and Longitude in the table using the given df \n",
    "\n",
    "        Parameters:\n",
    "            full_update_df (pd.DataFrame): df with columns named \"Latitude-Update\", \"Longitude-Update\", and \"RegionID\"\n",
    "            table_name (str): The name of the table to extract coordinate data from\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If the table does not exist\n",
    "        \"\"\"\n",
    "        with sqlite3.connect(self.db_path) as conn:\n",
    "            cursor = conn.cursor()\n",
    "\n",
    "            # Checks if table exists\n",
    "            cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "            valid_tables = {row[0] for row in cursor.fetchall()}\n",
    "            if table_name not in valid_tables:\n",
    "                raise ValueError(f\"Invalid table name: {table_name}\")\n",
    "\n",
    "            update_sql = f\"\"\"\n",
    "                UPDATE {table_name}\n",
    "                SET Latitude = ?, Longitude = ?\n",
    "                WHERE RegionID = ?\n",
    "            \"\"\"\n",
    "\n",
    "            update_data = [ # Creates a list of tuples with the latitude, longitude, and regionid from the passed in df\n",
    "                (row[\"Latitude-Update\"], row[\"Longitude-Update\"], row[\"RegionID\"])\n",
    "                for _, row in full_update_df.iterrows()\n",
    "            ]\n",
    "\n",
    "            cursor.executemany(update_sql, update_data) # Uses the list of tuples and updates the table with it\n",
    "            conn.commit()\n",
    "            \n",
    "        # Way used:\n",
    "        # weird_df = merged_df[(merged_df[\"Latitude\"] > 50) & (merged_df[\"StateName\"] != \"AK\")]\n",
    "        # weird_df\n",
    "        # for index, row in weird_df.iterrows():\n",
    "        #     lat, long = get_city_coords(f\"{row['RegionName']}, {row['City']}, California\")\n",
    "        #     if lat:\n",
    "        #         weird_df.at[index, \"Latitude\"] = lat\n",
    "        #     if long:\n",
    "        #         weird_df.at[index, \"Longitude\"] = long\n",
    "        # weird_df.loc[weird_df[\"RegionID\"] == 810129, \"Latitude\"] = 34.036232, \n",
    "        # weird_df.loc[weird_df[\"RegionID\"] == 810129, \"Longitude\"] = -117.613777\n",
    "        # weird_df\n",
    "        # update_table_coords(weird_df, \"Metro_Location_Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SQLite DB\n",
    "DB = DBManager()\n",
    "DB.safe_to_sqlite(df_data, \"Location_Data\")\n",
    "DB.safe_to_sqlite(df_values, \"ZHVI_Data\")\n",
    "DB.safe_to_sqlite(full_metro_loc, \"Metro_Location_Data\")\n",
    "DB.safe_to_sqlite(metro_time_sales, \"Metro_Sales_Data\")\n",
    "DB.safe_to_sqlite(metro_time_zhvi, \"Metro_ZHVI_Data\")\n",
    "DB.safe_to_sqlite(metro_time_sales_raw, \"Metro_Sales_Raw_Data\")\n",
    "\n",
    "# Adds indices to make the process more efficient\n",
    "with sqlite3.connect(SQLITE_DB_URL) as conn:\n",
    "    conn.execute(\"CREATE INDEX IF NOT EXISTS idx_housing_regionid ON Location_Data(RegionID);\")\n",
    "    conn.execute(\"CREATE INDEX IF NOT EXISTS idx_housing_regionid_date ON ZHVI_Data(RegionID, Date);\")\n",
    "    conn.execute(\"CREATE INDEX IF NOT EXISTS idx_metro_housing_regionid ON Metro_Location_Data(RegionID);\")\n",
    "    conn.execute(\"CREATE INDEX IF NOT EXISTS idx_metro_region_date_sales ON Metro_Sales_Data(RegionID, Date);\")\n",
    "    conn.execute(\"CREATE INDEX IF NOT EXISTS idx_metro_region_date_zhvi ON Metro_ZHVI_Data(RegionID, Date);\")\n",
    "    conn.execute(\"CREATE INDEX IF NOT EXISTS idx_metro_region_date_sales_raw ON Metro_Sales_Raw_Data(RegionID, Date);\")\n",
    "\n",
    "# DB.coordinates_to_json(\"Location_Data\") # Used to create the jsons, which are more feasible to send to other users, as opposed to the whole db\n",
    "# DB.coordinates_to_json(\"Metro_Location_Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metro_loc = DB.table_to_df(\"Metro_Location_Data\")\n",
    "metro_loc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metro_sales = DB.table_to_df(\"Metro_Sales_Data\")\n",
    "metro_sales.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metro_zhvi = DB.table_to_df(\"Metro_ZHVI_Data\")\n",
    "metro_zhvi.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metro_sales_raw = DB.table_to_df(\"Metro_Sales_Raw_Data\")\n",
    "metro_sales_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_df = DB.table_to_df(\"Location_Data\")\n",
    "location_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ZHVI_df = DB.select_date_table(\"ZHVI_Data\")\n",
    "ZHVI_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = DB.select_merged_table({\"ZHVI_Data\", \"Location_Data\"}, date=\"newest\")\n",
    "continental_df = merged_df[(merged_df[\"StateName\"] != \"HI\") & (merged_df[\"StateName\"] != \"AK\")] # Want to focus on lower 48\n",
    "continental_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_sales_raw_df = DB.select_merged_table((\"Metro_Sales_Raw_Data\", \"Metro_Location_Data\"), date=\"newest\")\n",
    "continental_sales_raw_df = merged_sales_raw_df[(merged_sales_raw_df[\"StateName\"] != \"HI\") & (merged_sales_raw_df[\"StateName\"] != \"AK\") & (merged_sales_raw_df[\"StateName\"] != \"USA\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlottingModels:\n",
    "    def plot_housing_map(df : pd.DataFrame, col : pd.Series, cluster : bool=False, n_clusters : int=10, value_name : str=None, figsize : tuple=(12, 10)):\n",
    "        \"\"\"\n",
    "        Plots housing data on a map of the continental US, with the option of clustering.\n",
    "        \n",
    "        Parameters:\n",
    "            df (pd.DataFrame): DataFrame containing housing data with Latitude and Longitude columns\n",
    "            col (pd.Series): Column from the passed in df that will be plotted\n",
    "            cluster (bool, default=False): If True, perform KMeans clustering on the data\n",
    "            n_clusters (int, default=10): Number of clusters for KMeans (only used if cluster=True) \n",
    "            value_name (str, optional): Name of the value being mapped\n",
    "            figsize (tuple, default=(12, 10)): Figure size for the plot\n",
    "            \n",
    "        Returns:\n",
    "            fig, ax (matplotlib figure and axis objects)\n",
    "        \"\"\"\n",
    "        # Create GeoDataFrame\n",
    "        gdf = gpd.GeoDataFrame(\n",
    "            df,\n",
    "            geometry=gpd.points_from_xy(df.Longitude, df.Latitude),\n",
    "            crs=\"EPSG:4326\"  # WGS 84 — standard lat/lon\n",
    "        )\n",
    "\n",
    "        gdf[\"RequestedCol\"] = col\n",
    "        \n",
    "        # Extract lat/lon explicitly (needed for clustering)\n",
    "        gdf[\"Latitude\"] = gdf.geometry.y\n",
    "        gdf[\"Longitude\"] = gdf.geometry.x\n",
    "\n",
    "        # Perform clustering if requested\n",
    "        if cluster:\n",
    "            # Normalize and Cluster\n",
    "            features = gdf[[\"Latitude\", \"Longitude\", \"RequestedCol\"]]\n",
    "            scaler = StandardScaler()\n",
    "            X_scaled = scaler.fit_transform(features)\n",
    "            \n",
    "            # Apply KMeans clustering\n",
    "            kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "            gdf[\"cluster\"] = kmeans.fit_predict(X_scaled)\n",
    "        \n",
    "        # Sets up Cartopy map\n",
    "        fig, ax = plt.subplots(figsize=figsize, subplot_kw={'projection': ccrs.PlateCarree()})\n",
    "        \n",
    "        # Adds Cartopy features\n",
    "        ax.add_feature(cfeature.LAND, facecolor='white')\n",
    "        ax.add_feature(cfeature.BORDERS, edgecolor='black')\n",
    "        ax.add_feature(cfeature.COASTLINE)\n",
    "        ax.add_feature(cfeature.STATES, edgecolor='gray', linewidth=0.5)\n",
    "        \n",
    "        # Restricts view to continental US\n",
    "        ax.set_extent([-130, -65, 24, 50], crs=ccrs.PlateCarree())\n",
    "        \n",
    "        if cluster:\n",
    "            # Plot with clusters\n",
    "            scatter = gdf.plot(\n",
    "                ax=ax,\n",
    "                column=\"cluster\",\n",
    "                cmap=\"tab10\",           # Distinct colors for clusters\n",
    "                legend=True,\n",
    "                markersize=20,\n",
    "                alpha=0.8,\n",
    "                transform=ccrs.PlateCarree()\n",
    "            )\n",
    "            if value_name:\n",
    "                new_title = f\"KMeans Clustering of {value_name} by Location and Value\"\n",
    "            else:\n",
    "                new_title = \"KMeans Clustering of Housing Prices by Location and Value\"\n",
    "        else:\n",
    "            # Calculate IQR bounds for better color scaling\n",
    "            Q1 = gdf[\"RequestedCol\"].quantile(0.25)\n",
    "            Q3 = gdf[\"RequestedCol\"].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            \n",
    "            vmin = Q1 - 1.5 * IQR\n",
    "            vmax = Q3 + 1.5 * IQR\n",
    "            \n",
    "            # Clamp values so outliers don't mess up the scale\n",
    "            vmin = max(vmin, gdf[\"RequestedCol\"].min())\n",
    "            vmax = min(vmax, gdf[\"RequestedCol\"].max())\n",
    "            \n",
    "            # Plot with continuous color\n",
    "            scatter = gdf.plot(\n",
    "                ax=ax,\n",
    "                column=\"RequestedCol\",\n",
    "                cmap=\"viridis\",\n",
    "                legend=True,\n",
    "                markersize=20,\n",
    "                alpha=0.7,\n",
    "                vmax=vmax,\n",
    "                vmin=vmin,\n",
    "                transform=ccrs.PlateCarree()\n",
    "            )\n",
    "            if value_name:\n",
    "                new_title = value_name\n",
    "            else:\n",
    "                new_title = \"Housing Prices\"\n",
    "        \n",
    "        # Set title with optional date\n",
    "        if df[\"Date\"].max():\n",
    "            ax.set_title(f\"{new_title} as of {df[\"Date\"].max()}\")\n",
    "        else:\n",
    "            ax.set_title(new_title)\n",
    "        \n",
    "        # Add credit for OpenStreetMap\n",
    "        fig.text(\n",
    "            .2, 0.01,\n",
    "            \"Coordinate Data from OpenStreetMap\", \n",
    "            fontsize=11, color='black',\n",
    "            ha='right', va='bottom'\n",
    "        )\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return fig, ax\n",
    "\n",
    "    def elbow_method(df : pd.DataFrame, col : str):\n",
    "        \"\"\"\n",
    "        Performs the elbow omethod on the requested column in df to determine optimal amount for clustering\n",
    "\n",
    "        Parameters:\n",
    "            df (pd.DataFrame): DataFrame with Latitude and Longitude, which will are features for clustering\n",
    "            col (str): Name of a column in the df which will have its values used as a third feature in the clustering\n",
    "        \"\"\"\n",
    "        if \"Latitude\" not in df.columns or \"Longitude\" not in df.columns:\n",
    "            print(\"Missing coordinate data.\")\n",
    "            return\n",
    "        \n",
    "        x = df.loc[:, [col, \"Latitude\", \"Longitude\"]]\n",
    "        wcss = []\n",
    "        for i in range(1, 50):\n",
    "            kmeans= KMeans(i)\n",
    "            kmeans.fit(x)\n",
    "            wcss_iter = kmeans.inertia_\n",
    "            wcss.append(wcss_iter)\n",
    "\n",
    "        number_clusters = range(1, 50)\n",
    "        plt.plot(number_clusters, wcss)\n",
    "        plt.title('Elbow')\n",
    "        plt.xlabel('Number of Clusters')\n",
    "        plt.ylabel('WCSS')\n",
    "\n",
    "    def plot_time_series(region_ids : list,\n",
    "                        time_series_table : str, \n",
    "                        location_table : str,\n",
    "                        value_column : str,\n",
    "                        id_column : str=\"RegionID\",\n",
    "                        name_column : str=\"RegionName\",\n",
    "                        date_column : str=\"Date\",\n",
    "                        legend : bool=True,\n",
    "                        figsize : tuple=(12, 6),\n",
    "                        title : str=None,\n",
    "                        y_label : str=None,\n",
    "                        db_path : str=SQLITE_DB_URL):\n",
    "        \"\"\"\n",
    "        Plot multiple time series from different regions on the same chart.\n",
    "        \n",
    "        Parameters:\n",
    "            region_ids (list): List of region IDs to plot\n",
    "            time_series_table (str): Name of the table containing time series data\n",
    "            location_table (str): Name of the table containing location data\n",
    "            value_column (str): Name of the column containing values to plot on y axis\n",
    "            id_column (str): Name of the column containing region IDs\n",
    "            name_column (str): Name of the column containing region names\n",
    "            date_column (str): Name of the column containing dates\n",
    "            legend (bool, default=True): Displays the legend when True, otherwise does not\n",
    "            figsize (tuple, default=(12,6)): Figure size of the plot\n",
    "            title (str, optional): Plot title (defaults to '{value_column} Over Time by Region' if not given)\n",
    "            y_label (str, optional): Y-axis label (defaults to value_column if not given)\n",
    "            db_path (str, default=SQLITE_DB_URL):\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If the table does not exist or required columns are missing\n",
    "        \"\"\"\n",
    "        # Create placeholders for SQL IN clause\n",
    "        placeholders = ','.join(['?' for _ in region_ids])\n",
    "        \n",
    "        # Construct the SQL query dynamically\n",
    "        query = f\"\"\"\n",
    "            SELECT ts.{date_column}, ts.{value_column}, loc.{name_column}, loc.{id_column}\n",
    "            FROM {time_series_table} ts\n",
    "            JOIN {location_table} loc USING ({id_column})\n",
    "            WHERE loc.{id_column} IN ({placeholders})\n",
    "        \"\"\"\n",
    "        with sqlite3.connect(db_path) as conn:\n",
    "            # Checks that the tables exist\n",
    "            cursor = conn.cursor()\n",
    "            cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "            valid_tables = {row[0] for row in cursor.fetchall()}\n",
    "            \n",
    "            for table in [time_series_table, location_table]:\n",
    "                if time_series_table not in valid_tables or location_table not in valid_tables:\n",
    "                    raise ValueError(f\"'{table}' does not exist in the database\")\n",
    "\n",
    "                # Checks if the expected columns are in their expected tables\n",
    "                if table == time_series_table:\n",
    "                    cursor.execute(f\"PRAGMA table_info({table});\")\n",
    "                    table_columns = {row[1] for row in cursor.fetchall()}\n",
    "                    \n",
    "                    for col in [id_column, date_column, value_column]:\n",
    "                        if col not in table_columns:\n",
    "                            raise ValueError(f\"'{col}' does not exist in '{table}'\")\n",
    "                        \n",
    "                elif table == location_table:\n",
    "                    cursor.execute(f\"PRAGMA table_info({table});\")\n",
    "                    table_columns = {row[1] for row in cursor.fetchall()}\n",
    "                    \n",
    "                    for col in [id_column, name_column]:\n",
    "                        if col not in table_columns:\n",
    "                            raise ValueError(f\"'{col}' does not exist in '{table}'\")\n",
    "                \n",
    "            all_data = pd.read_sql_query(query, conn, params=region_ids) # Fetch data\n",
    "        all_data[date_column] = pd.to_datetime(all_data[date_column], errors=\"coerce\")\n",
    "        \n",
    "        # Setup plot\n",
    "        plt.figure(figsize=figsize)\n",
    "        plt.gca().xaxis.set_major_locator(mdates.AutoDateLocator())\n",
    "        plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n",
    "        plt.xticks(rotation=45)\n",
    "\n",
    "        num_regions = len(region_ids)\n",
    "        colors = cm.viridis(np.linspace(0, 1, num_regions))\n",
    "\n",
    "        for i, region_id in enumerate(region_ids):\n",
    "            region_data = all_data[all_data[id_column] == region_id]\n",
    "            if region_data.empty:\n",
    "                continue\n",
    "            ts = region_data.set_index(date_column)[value_column]\n",
    "            plt.plot(ts, color=colors[i], label=region_data[name_column].iloc[0])\n",
    "\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel(y_label if y_label else value_column)\n",
    "        plt.title(title if title else f'{value_column} Over Time by Region')\n",
    "        if legend:\n",
    "            plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PlottingModels.plot_housing_map(continental_df, np.log10(continental_df[\"ZHVI\"]), value_name=\"ZHVI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PlottingModels.elbow_method(continental_df, \"ZHVI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Around 10 clusters, or maybe even a couple less than 10, seems to be the point where more clusters does not improve the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PlottingModels.plot_housing_map(continental_df, np.log10(continental_df[\"ZHVI\"]), cluster=True, value_name=\"ZHVI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PlottingModels.plot_housing_map(continental_sales_raw_df, continental_sales_raw_df[\"Median_Sales_Raw\"], value_name=\"Raw Median Sales Price\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PlottingModels.elbow_method(continental_sales_raw_df, \"Median_Sales_Raw\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also seems to take around 10 clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PlottingModels.plot_housing_map(continental_sales_raw_df, continental_sales_raw_df[\"Median_Sales_Raw\"], cluster=True, value_name=\"Raw Median Sales Price\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examined_id = 270957 # Upper east side, NY\n",
    "PlottingModels.plot_time_series([examined_id], \"ZHVI_Data\", \"Location_Data\", \"ZHVI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metro_ids_df = DB.table_to_df(\"Metro_Sales_Raw_Data\", cols=[\"RegionID\"]).drop_duplicates()\n",
    "all_metro_ids = all_metro_ids_df.iloc[:, 0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = DB.table_to_df(\"Metro_Sales_Raw_Data\")\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PlottingModels.plot_time_series(all_metro_ids, \"Metro_Sales_Data\", \"Metro_Location_Data\", \"Median_Sales\", legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_of_regions(df : pd.DataFrame, col : str, required_regions : list = None, max_rows : int=10):\n",
    "    \"\"\"\n",
    "    Gets a specified number of region ids from the given dataframe, based on the passed in col\n",
    "    \"\"\"\n",
    "    if \"Date\" not in df.columns:\n",
    "        print(\"Expected a date column\")\n",
    "        return None\n",
    "    \n",
    "    df = df[df[\"Date\"] == df[\"Date\"].max()] # Sort by most recent data\n",
    "\n",
    "    sorted_df = df.sort_values(col).reset_index(drop=True)\n",
    "\n",
    "    # Remove outliers\n",
    "    Q1 = sorted_df[col].quantile(0.25)\n",
    "    Q3 = sorted_df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    minimum = Q1 - 1.5 * IQR\n",
    "    maximum = Q3 + 1.5 * IQR\n",
    "\n",
    "    sorted_df = sorted_df[(sorted_df[col] > minimum) & (sorted_df[col] < maximum)]\n",
    "\n",
    "    min_row = sorted_df.iloc[[0]]\n",
    "    max_row = sorted_df.iloc[[-1]]\n",
    "    median_value = sorted_df[col].median()\n",
    "    median_row = sorted_df.iloc[[(sorted_df[col] - median_value).abs().argmin()]]\n",
    "    filtered_df = sorted_df[sorted_df['RegionID'].isin(required_regions)]\n",
    "\n",
    "    # Collect all indices to drop\n",
    "    drop_indices = set(filtered_df.index)  # all matched rows\n",
    "    drop_indices.update([min_row.index[0], max_row.index[0], median_row.index[0]])  # add the other specified rows\n",
    "\n",
    "    # Drop them\n",
    "    no_dupes_sorted_df = sorted_df.drop(drop_indices)\n",
    "\n",
    "    remaining_rows = max_rows - (len(sorted_df) - len(no_dupes_sorted_df))\n",
    "\n",
    "    # Sample rows\n",
    "    step = max(1, len(no_dupes_sorted_df) // (remaining_rows))\n",
    "    sampled_rows = no_dupes_sorted_df.iloc[::step].head(remaining_rows)\n",
    "\n",
    "    good_mix_of_regions = pd.concat([min_row, median_row, max_row, filtered_df, sampled_rows]).sort_index() # Sort again since sampled are just tacked on after the others\n",
    "    return good_mix_of_regions[\"RegionID\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_id = 102001 # Region ID of the USA as a whole in the data\n",
    "\n",
    "PlottingModels.plot_time_series(sample_of_regions(metro_sales_raw, \"Median_Sales_Raw\", [us_id]), \"Metro_Sales_Raw_Data\", \"Metro_Location_Data\", \"Median_Sales_Raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metro_merged_df = DB.select_merged_table({\"Metro_Sales_Data\", \"Metro_Location_Data\"}, key_value=us_id, cols=[\"Date\", \"Median_Sales\"])\n",
    "metro_merged_df[\"Date\"] = pd.to_datetime(metro_merged_df[\"Date\"], errors=\"coerce\")\n",
    "metro_merged_df.set_index(\"Date\", inplace=True)\n",
    "\n",
    "# Decompose into trend, seasonality, and residual\n",
    "decomposition = seasonal_decompose(metro_merged_df, model='multiplicative')\n",
    "decomposition.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_metro_merged_df = DB.select_merged_table({\"Metro_Sales_Raw_Data\", \"Metro_Location_Data\"}, key_value=us_id, cols=[\"Date\", \"Median_Sales_Raw\"])\n",
    "raw_metro_merged_df[\"Date\"] = pd.to_datetime(raw_metro_merged_df[\"Date\"], errors=\"coerce\")\n",
    "raw_metro_merged_df.set_index(\"Date\", inplace=True)\n",
    "\n",
    "# Decompose into trend, seasonality, and residual\n",
    "raw_decomposition = seasonal_decompose(raw_metro_merged_df, model='multiplicative')\n",
    "raw_decomposition.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Insight**\n",
    "1. After 2012, the trend line grows fairly steadily until 2020, where the trend line increases faster, before leveling off at 2022 and is seemingly increasing at a rate between what it was from 2012-2020 and what it was between 2020-2022. Overrall it shows a long term growth.\n",
    "2. There is a regularly repeating pattern, suggesting prices rise and fall consistently with the seasons.\n",
    "3. The data seems to be multiplicative since the variation increases over time, and since the trend and seasonality are both reasonably strong, more complex models are justified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Forecast:\n",
    "    def __init__(self, df : pd.DataFrame, percentage : float=0.8):\n",
    "        \"\"\"\n",
    "        Initializes the forecasting class with a dataframe and train/test split percentage\n",
    "        \n",
    "        Parameters:\n",
    "            df (pd.DataFrame): A dataframe with time series data to forecast\n",
    "            percentage (float, default=0.8): Percentage of data to use for training\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.train_size = int(len(df) * percentage)\n",
    "        self.train, self.test = df.iloc[:self.train_size], df.iloc[self.train_size:] # The first 80% of data is for training (by default), the remaining is for testing since past predicts future\n",
    "        self.forecasts = {}\n",
    "        self.metrics = {}\n",
    "    \n",
    "    def fit_arima(self, order : tuple=(5, 1, 0), model_name : str=\"ARIMA\"):\n",
    "        \"\"\"\n",
    "        Fits an ARIMA model to the training data\n",
    "        \n",
    "        Parameters\n",
    "            order (tuple): ARIMA order parameters (p, d, q)\n",
    "            model_name (str): Name to identify this model in results\n",
    "        \"\"\"\n",
    "        model = ARIMA(self.train, order=order)\n",
    "        model_fit = model.fit()\n",
    "        forecast = model_fit.forecast(steps=len(self.test))\n",
    "        \n",
    "        # Stores the forecast results\n",
    "        self.forecasts[model_name] = forecast\n",
    "        self._calculate_metrics(model_name)\n",
    "        \n",
    "        return forecast\n",
    "    \n",
    "    def fit_holtwinters(self, trend : str=\"add\", seasonal : str=\"add\", seasonal_periods : int=12, model_name : str=\"Holt-Winters\"):\n",
    "        \"\"\"\n",
    "        Fits a Holt-Winters model to the training data\n",
    "        \n",
    "        Parameters:\n",
    "            trend (str, default=\"add\"): Type of trend component ('add', 'mul', or None)\n",
    "            seasonal (str, default=\"add\"): Type of seasonal component ('add', 'mul', or None)\n",
    "            seasonal_periods (int): Number of time steps in a seasonal period\n",
    "            model_name (str): Name to identify this model in results\n",
    "        \"\"\"\n",
    "        model = ExponentialSmoothing(self.train, trend=trend, seasonal=seasonal, \n",
    "                                     seasonal_periods=seasonal_periods)\n",
    "        model_fit = model.fit()\n",
    "        forecast = model_fit.forecast(len(self.test))\n",
    "        \n",
    "        # Stores the forecast results\n",
    "        self.forecasts[model_name] = forecast\n",
    "        self._calculate_metrics(model_name)\n",
    "        \n",
    "        return forecast\n",
    "    \n",
    "    def fit_sarima(self, order : tuple=(1,1,1), seasonal_order : tuple=(1,1,1,12), model_name : str=\"SARIMA\"):\n",
    "        \"\"\"\n",
    "        Fits a SARIMA model to the training data\n",
    "        \n",
    "        Parameters:\n",
    "            order (tuple): SARIMA order parameters (p, d, q)\n",
    "            seasonal_order (tuple): SARIMA seasonal_order parameters (P, D, Q, S)\n",
    "            model_name (str): Name to identify this model in results\n",
    "        \"\"\"\n",
    "        model = SARIMAX(self.train, order=order, seasonal_order=seasonal_order)\n",
    "        \n",
    "        model_fit = model.fit()\n",
    "        forecast = model_fit.forecast(len(self.test))\n",
    "\n",
    "        # Store forecast results\n",
    "        self.forecasts[model_name] = forecast\n",
    "        self._calculate_metrics(model_name)\n",
    "        \n",
    "        return forecast\n",
    "    \n",
    "    def _calculate_metrics(self, model_name : str):\n",
    "        \"\"\"\n",
    "        Calculate error metrics for a given model's forecast\n",
    "        \n",
    "        Parameters:\n",
    "            model_name (str): The name of the model as it is stored in self.forecasts \n",
    "        \"\"\"\n",
    "        forecast = self.forecasts[model_name]\n",
    "        \n",
    "        mae = mean_absolute_error(self.test, forecast)\n",
    "        mse = mean_squared_error(self.test, forecast)\n",
    "        rmse = np.sqrt(mse)\n",
    "        \n",
    "        self.metrics[model_name] = {\n",
    "            \"MAE\": mae,\n",
    "            \"MSE\": mse,\n",
    "            \"RMSE\": rmse\n",
    "        }\n",
    "    \n",
    "    def plot_forecast(self, model_name : str, figsize : tuple=(12, 6), linestyle : str=\"--\"):\n",
    "        \"\"\"\n",
    "        Plot a specific forecast against actual data\n",
    "        \n",
    "        Parameters:\n",
    "            model_name (str): Name of the forecast to plot as stored in self.forecasts\n",
    "            figsize (tuple): Figure size for the plot\n",
    "            linestyle (str): Line style for the forecast line\n",
    "        \"\"\"\n",
    "        if model_name not in self.forecasts:\n",
    "            raise ValueError(f\"Model {model_name} has not been fitted yet\")\n",
    "            \n",
    "        forecast = self.forecasts[model_name]\n",
    "        \n",
    "        plt.figure(figsize=figsize)\n",
    "        plt.plot(self.train, label=\"Training Data\")\n",
    "        plt.plot(self.test, label=\"Actual Test Data\")\n",
    "        plt.plot(self.test.index, forecast, label=f\"{model_name} Forecast\", linestyle=linestyle)\n",
    "        plt.legend()\n",
    "        plt.title(f\"{model_name} Model Forecasting\")\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_all_forecasts(self, figsize : tuple=(12, 6)):\n",
    "        \"\"\"\n",
    "        Plots all created forecasts at once for comparison\n",
    "        \n",
    "        Parameters:\n",
    "            figsize (tuple): Figure size of the plot\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=figsize)\n",
    "        plt.plot(self.train, label=\"Training Data\", color=\"blue\")\n",
    "        plt.plot(self.test, label=\"Actual Test Data\", color=\"green\")\n",
    "        \n",
    "        colors = [\"red\", \"purple\", \"orange\", \"brown\", \"pink\"]\n",
    "        linestyles = [\"--\", \"-.\", \":\", \"-\"]\n",
    "        \n",
    "        for i, (name, forecast) in enumerate(self.forecasts.items()):\n",
    "            plt.plot(self.test.index, forecast, \n",
    "                     label=f\"{name} Forecast\", \n",
    "                     linestyle=linestyles[i % len(linestyles)],\n",
    "                     color=colors[i % len(colors)])\n",
    "        \n",
    "        plt.legend()\n",
    "        plt.title(\"Forecasting Models Comparison\")\n",
    "        plt.show()\n",
    "    \n",
    "    def print_metrics(self, model_name : str=None):\n",
    "        \"\"\"\n",
    "        Print metrics for a specific model or all models\n",
    "        \n",
    "        Parameters:\n",
    "            model_name (str, default=None): Name of the model to print metrics for (if None, print all)\n",
    "        \"\"\"\n",
    "        if model_name is not None:\n",
    "            if model_name not in self.metrics:\n",
    "                raise ValueError(f\"Model {model_name} has not been fitted yet\")\n",
    "            \n",
    "            metrics = self.metrics[model_name]\n",
    "            print(f\"=== {model_name} Metrics ===\")\n",
    "            print(f\"{model_name} MAE: {metrics['MAE']:.4f}\")\n",
    "            print(f\"{model_name} MSE: {metrics['MSE']:.4f}\")\n",
    "            print(f\"{model_name} RMSE: {metrics['RMSE']:.4f}\")\n",
    "            print()\n",
    "            \n",
    "        else:\n",
    "            for name, metrics in self.metrics.items():\n",
    "                print(f\"=== {name} Metrics ===\")\n",
    "                print(f\"{name} MAE: {metrics['MAE']:.4f}\")\n",
    "                print(f\"{name} MSE: {metrics['MSE']:.4f}\")\n",
    "                print(f\"{name} RMSE: {metrics['RMSE']:.4f}\")\n",
    "                print()\n",
    "    \n",
    "    def get_best_model(self, metric : str=\"MSE\"):\n",
    "        \"\"\"\n",
    "        Get the name of the best performing model based on a specific metric\n",
    "        \n",
    "        Parameters:\n",
    "            metric (str): Metric to use for comparison ('MAE', 'MSE', or 'RMSE')\n",
    "            \n",
    "        Returns:\n",
    "            Name of the best model (str)\n",
    "        \n",
    "        Raises:\n",
    "            ValueError: If no models have been forecasted/fitted\n",
    "        \"\"\"\n",
    "        if not self.metrics:\n",
    "            raise ValueError(\"No models have been fitted yet\")\n",
    "            \n",
    "        best_model = min(self.metrics.keys(), \n",
    "                         key=lambda model: self.metrics[model][metric])\n",
    "        \n",
    "        return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast = Forecast(metro_merged_df)\n",
    "forecast.fit_arima()\n",
    "forecast.fit_holtwinters(seasonal_periods=52)\n",
    "forecast.fit_sarima(seasonal_order=(1,1,1,52))\n",
    "forecast.plot_all_forecasts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast.print_metrics()\n",
    "for metric in [\"MAE\", \"MSE\", \"RMSE\"]:\n",
    "    print(f\"The model with the best {metric} score is {forecast.get_best_model(metric=metric)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_forecast = Forecast(raw_metro_merged_df)\n",
    "raw_forecast.fit_arima()\n",
    "raw_forecast.fit_holtwinters(trend=\"add\", seasonal=\"mul\", seasonal_periods=12)\n",
    "raw_forecast.fit_sarima(seasonal_order=(1,1,1,12))\n",
    "raw_forecast.plot_all_forecasts()\n",
    "forecast.print_metrics()\n",
    "for metric in [\"MAE\", \"MSE\", \"RMSE\"]:\n",
    "    print(f\"The model with the best {metric} score is {forecast.get_best_model(metric=metric)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
